# Simplified-gradient-descent
Optimization technique: Implementation of Gradient descent from scratch.

- The purpose of this tutorial is to give users an overview of the algorithm and how it works. 

- Gradient descent is one of the most known optimization technique in machine learning. This technique helps in ascertaining the better estimates for the weights of the model in order to minimize the cost function values.

### Gradient descent 

- This is an optimization technique that search for better estimates for the parameters in order to minimize the cost function.

- With gradient descent there two major parameters to tune which can be hyperparameter tuning. Those parameters are: 
  - Number of iteration 
  - learning rate alpha
- We initialize the weights using the normal distribution rather than setting the parameters to zeros. 
- The algorithm converges to the global minimum after some number of iterations. The update of the parameters is done after each iteration perform. 
- The detail of the implementation has been explained in the notebook. 
- In the context of our work we should have scale the input features but we did not do that in our implementation. We intend to consider that in our future works. 

Here is the paper source: [Reference](http://cs229.stanford.edu/notes2019fall/cs229-notes1.pdf)

- All suggestions and corrections are welcome. 
